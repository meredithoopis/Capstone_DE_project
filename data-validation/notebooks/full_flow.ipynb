{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import great_expectations as gx \n",
    "import pandas as pd \n",
    "\n",
    "context = gx.get_context()\n",
    "context = context.convert_to_file_context()\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = pd.read_parquet(\"../data/yellow_tripdata/yellow_tripdata_2023-01.parquet\")\n",
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get some first insights about the data\n",
    "sample_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Connect to a data source\n",
    "validator = context.sources.pandas_default.read_parquet(\n",
    "    \"../data/yellow_tripdata/yellow_tripdata_2023-01.parquet\"\n",
    ")\n",
    "\n",
    "# Let's assume we are ok with NaN in the other columns, except `passenger_count`\n",
    "# After running the following code, you can see we have 71743 NaN values\n",
    "# You can verify this by using this code (sample_data[sample_data[\"passenger_count\"].isnull()].shape[0])/sample_data.shape[0]\n",
    "validator.expect_column_values_to_not_be_null(\"passenger_count\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.expect_column_values_to_be_between(\"trip_distance\", min_value=0, max_value=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(validator.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.save_expectation_suite(discard_failed_expectations=False)\n",
    "\n",
    "# Define the checkpoint\n",
    "checkpoint = context.add_or_update_checkpoint(\n",
    "    name=\"yellow_tripdata_checkpoint\",\n",
    "    validator=validator\n",
    ")\n",
    "\n",
    "# Get the result after validation\n",
    "checkpoint_result = checkpoint.run()\n",
    "\n",
    "# Quick view on the validation result\n",
    "context.view_validation_result(checkpoint_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.build_data_docs()\n",
    "context.open_data_docs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A folder as a data source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.sources.add_pandas_filesystem(\n",
    "    name=\"my_ds\", base_directory=\"../data/yellow_tripdata/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ds = context.datasources[\"my_ds\"]\n",
    "\n",
    "my_batching_regex = \"yellow_tripdata_2023-.*.parquet\"\n",
    "\n",
    "# Create the data asset (as one or more files from our data source)\n",
    "my_asset = my_ds.add_parquet_asset(\n",
    "    name=\"my_tripdata_data_asset\", batching_regex=my_batching_regex\n",
    ")\n",
    "\n",
    "# Define a Batch Request to include all batches in the available data set\n",
    "my_batch_request = my_asset.build_batch_request()\n",
    "batches = my_asset.get_batch_list_from_batch_request(my_batch_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in batches:\n",
    "    print(batch.batch_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.add_or_update_expectation_suite(\"my_asset_expectation_suite\")\n",
    "\n",
    "asset_validator = context.get_validator(\n",
    "    batch_request=my_batch_request,\n",
    "    expectation_suite_name=\"my_asset_expectation_suite\",\n",
    ")\n",
    "asset_validator.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the same expectations as the single-file\n",
    "asset_validator.expect_column_values_to_not_be_null(\"passenger_count\")\n",
    "asset_validator.expect_column_values_to_be_between(\"trip_distance\", min_value=0, max_value=100) \n",
    "asset_validator.save_expectation_suite(discard_failed_expectations=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = context.add_or_update_checkpoint(\n",
    "    name=\"yellow_tripdata_asset_checkpoint\",\n",
    "    validator=asset_validator\n",
    ")\n",
    "\n",
    "# Get the result after validation\n",
    "checkpoint_result = checkpoint.run()\n",
    "\n",
    "# Quick view on the validation result\n",
    "context.view_validation_result(checkpoint_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Database as the datasource \n",
    "datasource_name = \"my_postgresql_ds\"\n",
    "my_connection_string = (\n",
    "    \"postgresql+psycopg2://helen:helen@localhost:5432/helen\"\n",
    ")\n",
    "\n",
    "datasource = context.sources.add_postgres(\n",
    "    name=datasource_name, connection_string=my_connection_string\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hn2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
